// src/lib/llm.ts - KOMPLETT OPTIMIERT
import Groq from 'groq-sdk';

const groq = new Groq({
  apiKey: process.env.GROQ_API_KEY,
});

export async function callLLM(prompt: string, context: string = "", dialogMode: boolean = false): Promise<string> {
  try {
    const systemMessage = dialogMode 
      ? `Du bist ein professioneller Geb√§ude-Energieberater und f√ºhrst strukturierte Interviews durch.

WICHTIGE REGELN:
- Best√§tige IMMER die Antwort des Nutzers positiv
- Stelle danach die n√§chste Frage klar und deutlich
- Verwende deutsche Sprache
- Halte Antworten kurz und strukturiert (2-4 S√§tze)
- Sei freundlich aber fokussiert
- Folge den Anweisungen im User-Prompt exakt

FORMAT:
1. Best√§tigung der Antwort
2. N√§chste Frage (falls vorhanden)
3. Optional: Kurzer Hinweis

Antworte VOLLST√ÑNDIG und befolge die Anweisungen genau.`

      : `Du bist ein Experte f√ºr Geb√§udeformulare und Energieberatung.

WICHTIGE REGELN:
- Antworte hilfreich und spezifisch auf Deutsch
- Nutze den gegebenen Kontext intelligent
- Sei konkret und l√∂sungsorientiert
- Verwende eine freundliche, professionelle Sprache
- Halte Antworten fokussiert und n√ºtzlich (2-5 S√§tze)

Beantworte die konkrete Frage des Nutzers basierend auf dem Kontext.`;

    console.log('ü§ñ LLM Call:', { 
      dialogMode, 
      promptLength: prompt.length, 
      contextLength: context.length 
    })

    const completion = await groq.chat.completions.create({
      messages: [
        {
          role: "system",
          content: systemMessage
        },
        {
          role: "user", 
          content: context ? `${context}\n\n${prompt}` : prompt
        }
      ],
      model: "llama3-8b-8192",
      temperature: dialogMode ? 0.4 : 0.6, // Niedriger f√ºr konsistente Dialog-F√ºhrung
      max_tokens: 1500, // Mehr Tokens f√ºr vollst√§ndige Antworten
      top_p: 0.85, // Fokussiertere, qualitativ bessere Antworten
      frequency_penalty: 0.2, // Weniger Wiederholungen
      presence_penalty: 0.1, // Mehr Variationen
    });

    const response = completion.choices[0]?.message?.content || "Keine Antwort erhalten";
    
    console.log('‚úÖ LLM Response:', { 
      responseLength: response.length,
      model: "llama3-8b-8192"
    })
    
    return response;
    
  } catch (error) {
    console.error('‚ùå LLM Error:', error);
    
    // Detaillierte Fehlerbehandlung
    if (error instanceof Error) {
      if (error.message.includes('API key')) {
        throw new Error('GROQ API-Schl√ºssel ung√ºltig oder fehlt');
      } else if (error.message.includes('rate limit')) {
        throw new Error('GROQ Rate-Limit erreicht - versuchen Sie es sp√§ter erneut');
      } else if (error.message.includes('model')) {
        throw new Error('GROQ Modell nicht verf√ºgbar');
      }
    }
    
    throw new Error('LLM-Service tempor√§r nicht verf√ºgbar');
  }
}