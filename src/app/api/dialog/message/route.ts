// src/app/api/dialog/message/route.ts - KOMPLETT √úBERARBEITET
import { NextRequest, NextResponse } from 'next/server'
import { callLLM } from '@/lib/llm'

interface DialogSession {
  sessionId: string
  questions: string[]
  answers: { [key: string]: string }
  currentQuestionIndex: number
  context: string
}

const sessions = new Map<string, DialogSession>()

export async function POST(request: NextRequest) {
  try {
    const { message, session_id, questionIndex } = await request.json()
    
    console.log('üí¨ Dialog message:', { message, session_id, questionIndex })
    
    // Session abrufen oder erstellen
    let session = sessions.get(session_id)
    
    if (!session) {
      session = {
        sessionId: session_id,
        questions: [
          "Welche Geb√§udeseite soll haupts√§chlich saniert werden?",
          "Welches D√§mmmaterial ist f√ºr Ihr Vorhaben vorgesehen?", 
          "Wurden bereits andere energetische Ma√ünahmen am Geb√§ude durchgef√ºhrt?",
          "Handelt es sich um eine freiwillige Modernisierung oder besteht eine gesetzliche Verpflichtung?"
        ],
        answers: {} as { [key: string]: string },
        currentQuestionIndex: questionIndex || 0,
        context: "Mehrfamilienhaus Baujahr 1965, WDVS-Sanierung Eingangsfassade S√ºdseite, 140mm Mineralwolle, √ñlheizung, Mieterin EG rechts 57,5m¬≤"
      }
    }
    
    // Antwort speichern
    const questionKey = `question_${session.currentQuestionIndex + 1}`
    session.answers[questionKey] = message
    
    // N√§chste Frage bestimmen
    const nextQuestionIndex = session.currentQuestionIndex + 1
    const isLastQuestion = nextQuestionIndex >= session.questions.length
    const nextQuestion = isLastQuestion ? null : session.questions[nextQuestionIndex]
    
    // PR√ÑZISER LLM-PROMPT
    const llmPrompt = isLastQuestion 
      ? `Der Nutzer hat gerade die letzte Frage beantwortet: "${message}"

AUFGABE: 
1. Best√§tige die Antwort h√∂flich
2. Gratuliere zur Vervollst√§ndigung 
3. Fasse die 4 gesammelten Antworten kurz zusammen
4. Sage, dass die Daten erfasst wurden

ANTWORTE NUR auf Deutsch, freundlich und professionell.`
      
      : `Der Nutzer hat auf die Frage "${session.questions[session.currentQuestionIndex]}" geantwortet: "${message}"

SZENARIO: ${session.context}

AUFGABE:
1. Best√§tige die Antwort kurz und positiv
2. Stelle dann die n√§chste Frage: "${nextQuestion}"
3. Gib bei Bedarf einen hilfreichen Hinweis zum Szenario

ANTWORTE NUR auf Deutsch, strukturiert und freundlich.`

    try {
      const llmResponse = await callLLM(
        llmPrompt,
        '', // Kein extra Kontext - alles ist im Prompt
        true // Dialog-Modus
      )
      
      console.log('‚úÖ Dialog LLM response generated')
      
      // Session aktualisieren
      session.currentQuestionIndex = nextQuestionIndex
      sessions.set(session_id, session)
      
      return NextResponse.json({
        response: llmResponse,
        session_id: session_id,
        question_index: nextQuestionIndex,
        dialog_complete: isLastQuestion,
        answers_collected: session.answers
      })
      
    } catch (llmError) {
      console.error('‚ùå Dialog LLM failed:', llmError)
      
      // Fallback bei LLM-Ausfall
      const fallbackResponse = generateDialogFallback(
        message, 
        session.currentQuestionIndex, 
        nextQuestion, 
        isLastQuestion,
        session.answers
      )
      
      // Session trotzdem aktualisieren
      session.currentQuestionIndex = nextQuestionIndex
      sessions.set(session_id, session)
      
      return NextResponse.json({
        response: fallbackResponse,
        session_id: session_id,
        question_index: nextQuestionIndex,
        dialog_complete: isLastQuestion,
        answers_collected: session.answers
      })
    }
    
  } catch (error) {
    console.error('‚ùå Dialog API error:', error)
    return NextResponse.json({
      response: "Entschuldigung, es gab einen Fehler. K√∂nnen Sie Ihre Antwort wiederholen?",
      session_id: "error"
    }, { status: 500 })
  }
}

function generateDialogFallback(
  message: string, 
  currentQuestionIndex: number, 
  nextQuestion: string | null, 
  isLastQuestion: boolean,
  allAnswers: { [key: string]: string }
): string {
  
  if (isLastQuestion) {
    return `üéâ **Ausgezeichnet!** Sie haben alle Fragen beantwortet.

**Ihre Angaben:**
- Sanierungsbereich: ${allAnswers['question_1'] || 'Eingangsfassade'}
- D√§mmmaterial: ${allAnswers['question_2'] || 'Mineralwolle'}  
- Vorherige Ma√ünahmen: ${allAnswers['question_3'] || 'Keine'}
- Rechtliche Basis: ${message}

‚úÖ **Ihre Daten wurden erfolgreich erfasst!**`
  }
  
  const confirmations = [
    `‚úÖ **"${message}"** wurde als Sanierungsbereich erfasst.`,
    `‚úÖ **"${message}"** als D√§mmmaterial notiert.`,
    `‚úÖ **"${message}"** zu vorherigen Ma√ünahmen erfasst.`,
    `‚úÖ **"${message}"** zur rechtlichen Einordnung gespeichert.`
  ]
  
  return `${confirmations[currentQuestionIndex]}

**N√§chste Frage (${currentQuestionIndex + 2}/4):** ${nextQuestion}

üí° *Nutzen Sie die Informationen aus Ihrem Szenario zur Beantwortung.*`
}